### DACON
- 모델 : 각 컬럼을 예측하는 모델을 따로 생성하는 시도.
    - 한 번에 4개의 컬럼을 예측하는 모델보다 성능 떨어졌음
- 전처리 : src 의 최댓값에 해당하는 컬럼에만 1을 부여하고 나머지는 0을 부여하는 원-핫 인코딩 방식 시도.
    - 딱히 성능에 변동이 없었다.
#### DACON 고찰 - 왜 성능이 잘 안나왔을까?
- 전처리
    - 측정 스펙트럼과 광원 스펙트럼의 연결을 매끄럽지 못하게 한 점
    - 알맞은 scaler를 사용하지 못한 점
        - 결국 측정 스펙트럼을 전처리 할 때는 로그를 씌운 후 StandardScaler를 쓰는게 가장 성능이 좋았는데, 측정스펙트럼은 워낙 값이 작고 변화가 커서 스케일링을 한거라, 광원 스펙트럼도 전처리를 해야하는지 감이 안잡혔다.
    - 성능에 전처리를 맞춘 점
        - val_loss와 val_mae의 값에 따라 전처리를 줏대없이 하느라, 정작 데이터가 어떤 의미를 가지기 때문에 어떤 전처리를 시행한다는 타당성 없이 휘둘렸다는 점이 아쉽다.
- 모델
    - 여러가지 모델을 사용하지 않고 keras 네트워크만 사용하여 모델을 구성한 점
    - 그냥 모델에 훈련데이터만 때려박으면 알아서 학습되는게 아닌데 너무 안일했던 점
    - 데이터의 특성에 따라 선호되는 활성화함수나 손실함수가 있다는 사실을 간과한 점
#### 발전 방향
- 머신러닝과 딥러닝의 보다 깊이있는 학습이 필요함을 느꼈다.
- 무조건 딥러닝이 제일 좋은 학습 방법은 아님을 느꼈다.
    - 기존 sklearn에서 나온 여러가지 회귀모델도 성능을 더 올릴 수 있었을 것이다.


```python
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, MinMaxScaler


# In[2]:


train_data = pd.read_csv('./data/train.csv')
test_data = pd.read_csv('./data/test.csv')


# In[3]:


train_dst = train_data.loc[:,'650_dst':'990_dst']


# In[4]:


removed_index = train_dst[train_dst.isnull().sum(axis=1)>10].index
removed_index


# In[5]:


train_data.drop(removed_index, inplace=True)
train_data.reset_index(drop=True, inplace=True)


# In[6]:


def src_dst(train_data, test_data):
    train_src = train_data.filter(regex='src$', axis=1)
    test_src = test_data.filter(regex='src$', axis=1)
    
    train_dst = train_data.filter(regex='dst$', axis=1)
    test_dst = test_data.filter(regex='dst$', axis=1)
    train_dst = train_dst.apply(lambda x: x * train_data['rho']**2)
    test_dst = test_dst.apply(lambda x: x * test_data['rho']**2)
    
    train_dst.replace(0, np.nan, inplace=True)
    train_dst = train_dst.interpolate(method='linear', axis=1)
    
    test_dst.replace(0, np.nan, inplace=True)
    test_dst = test_dst.interpolate(method='linear', axis=1)
    
#     #... dst log scaling
    train_dst = train_dst.applymap(np.log)
    test_dst = test_dst.applymap(np.log)
    
    train_dst.replace(-np.inf, np.nan, inplace=True)
    test_dst.replace(-np.inf, np.nan, inplace=True)

    train_dst = train_dst.interpolate(method='linear', axis=1)
    test_dst= test_dst.interpolate(method='linear', axis=1)
    
    train_dst = train_dst.T.bfill().T
    test_dst = test_dst.T.bfill().T

    return {'train':{'src':train_src, 'dst':train_dst},
            'test':{'src':test_src, 'dst':test_dst}}


# ### ★ 광원 스펙트럼의 특정 파장의 빛의 세기가 0이더라도 
# ### 측정 스펙트럼에서 해당 파장의 빛의 세기가 있을 수 있다.★

# In[7]:


def dst_scaler(train_dst, test_dst):
    scaler = StandardScaler()
#     scaler = MinMaxScaler()
    columns_ = train_dst.columns
    scaler.fit(train_dst)
    train_dst = scaler.transform(train_dst)
    test_dst = scaler.transform(test_dst)
    
    train_dst_scaled = pd.DataFrame(train_dst, columns = columns_)
    test_dst_scaled = pd.DataFrame(test_dst, columns = columns_)  
    
    return {'train':train_dst_scaled, 'test':test_dst_scaled}


# In[9]:


#
import tensorflow as tf
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units = 420, activation='sigmoid', input_shape=(35,)),
        tf.keras.layers.Dense(units = 210, activation='relu'),
        tf.keras.layers.Dense(units = 140, activation='relu'),
        tf.keras.layers.Dense(units = 70, activation='relu'),
        tf.keras.layers.Dense(units = 16, activation='relu'),
#         tf.keras.layers.Dense(units = 4),
        tf.keras.layers.Dense(units = 1)
    ])

    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    return model


# In[10]:


src_dsts = src_dst(train_data, test_data)
train_src = src_dsts['train']['src']
test_src = src_dsts['test']['src']

# scaler2 = StandardScaler()
# train_src = pd.DataFrame(scaler2.fit_transform(train_src), columns = train_src.columns)
# test_src = pd.DataFrame(scaler2.transform(test_src),columns = test_src.columns)

dsts = dst_scaler(src_dsts['train']['dst'], src_dsts['test']['dst'])
train_dst = dsts['train']
test_dst = dsts['test']


# In[11]:


train_src_zeros = np.zeros(train_src.shape)
train_src_zeros


# In[12]:


for i in range(len(train_src)):
# i = 0
    train_src_zeros[i, np.argmax(train_src.loc[i])] = 1


# In[18]:


train_dst.head().T.plot()


# In[26]:


import os

CP_SAVE_FOLDER_PATH = './model_na_v1/'
if not os.path.exists(CP_SAVE_FOLDER_PATH):
    os.mkdir(CP_SAVE_FOLDER_PATH)

checkpoint_path = CP_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.ckpt'

# 체크포인트 콜백 만들기
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                 save_weights_only=True,
                                                 save_best_only=True,
                                                 verbose=1)

model = create_model()

# model.fit(train_dst_log.values,
model.fit([train_dst.values, train_src_zeros],
# model.fit([train_src_zeros, train_dst.values],
# model.fit([train_src.values, train_dst.values],
# model.fit(train_dst.values,
          train_data['na'].values,
#           train_data[['hhb','hbo2', 'ca', 'na']].values, 
          validation_split=0.2, shuffle=True, epochs=1000, batch_size=32, verbose=0,
          callbacks=[cp_callback])


# In[15]:


get_ipython().system('ls {CP_SAVE_FOLDER_PATH}')


# In[27]:


# CP_SAVE_FOLDER_PATH = './model_hhb_v1/'
latest = tf.train.latest_checkpoint(CP_SAVE_FOLDER_PATH)
latest


# In[17]:



# model = create_model()
# model.load_weights(latest)


# In[141]:


# pred = model.predict([test_dst.values, test_src.values])
# pred


# In[38]:


hhb_path = './model_hhb_v1/'    # 5.2038
hbo2_path = './model_hbo2_v1/'    # 0.9065
ca_path = './model_ca_v1/'    # 7.1043
na_path = './model_na_v1/'    # 2.5474

hhb_cp = tf.train.latest_checkpoint(hhb_path)
hbo2_cp = tf.train.latest_checkpoint(hbo2_path)
ca_cp = tf.train.latest_checkpoint(ca_path)
na_cp = tf.train.latest_checkpoint(na_path)

hhb_model = create_model()
hbo2_model = create_model()
ca_model = create_model()
na_model = create_model()

hhb_model.load_weights(hhb_cp)
hbo2_model.load_weights(hbo2_cp)
ca_model.load_weights(ca_cp)
na_model.load_weights(na_cp)

hhb_pred = hhb_model.predict([test_dst.values, test_src.values]).reshape(-1,)
hbo2_pred = hbo2_model.predict([test_dst.values, test_src.values]).reshape(-1,)
ca_pred = ca_model.predict([test_dst.values, test_src.values]).reshape(-1,)
na_pred = na_model.predict([test_dst.values, test_src.values]).reshape(-1,)


# In[39]:


hhb_pred


# In[41]:


subm = pd.DataFrame({'hhb':hhb_pred, 'hbo2':hbo2_pred, 'ca':ca_pred, 'na':na_pred})
subm['id'] = test_data['id']

subm = subm[['id','hhb','hbo2','ca','na']]
subm


# In[142]:


# subm = pd.DataFrame(dict(zip(('hhb','hbo2','ca','na'), zip(*pred))))
# subm['id'] = test_data['id']

# subm = subm[['id','hhb','hbo2','ca','na']]
# subm


# In[42]:


subm.to_csv('./subm_v8.csv', encoding='utf-8', index=False)


```