## Tensorflow 2.0 Programming
## **Ch03**
```python
# tensorflow version 확인
import tensorflow as tf
print(tf.__version__)
```

### 난수생성
`tf.random.uniform(shape, min, max)`  
- 최댓값과 최솟값 사이의 값이 추출될 확률이 균일한 균일분포에서 난수 추출    


`tf.random.normal(shape, mean, std)`  
- 정규분포에서 난수추출

```python
# 난수생성
rand = tf.random.uniform([1],0,1)
print(rand)
```

### 뉴런 만들기
- 입력 -> 가중치 -> 활성화함수 -> 촐력
    - 가장 간단한 형태의 뉴런. 입력에 가중치를 곱한뒤 활성화함수를 취해 출력을 얻는다.

```python
# 시그모이드 함수를 파이썬으로 구현
import math
def sigmoid(x):
    return 1/(1+math.exp(-x))

# 입력이 1일 때 기대출력이 0이 되는 뉴런을 만들어보자
x = 1
y = 0
w = tf.random.normal([1],0,1)
output = sigmoid(x*w)
print(output)
# output = 0.44
# error = 0 - 0.44 = -0.44
```

### 경사하강법(Gradient Descent)
- 뉴런의 학습은 에러가 0에 가까워지게 하는것 -> w값을 변화시키는 방법을 사용
- 경사하강법 : `w = w + x*alpha*error`
    - alpha : 학습률 
        - w를 업데이트 하는 정도
        - 큰 값 설정시 학습은 빠르지만 과도한 학습으로 적정치 벗어날 수 있음
        - 작은 값 설정시 학습속도가 너무 느려짐

```python
# 경사하강법이 효과를 발휘하는지 확인해보자
# alpha값은 0.1로 설정했다.
for i in range(1000):
    output = sigmoid(x*w)
    error = y-output
    w = w + x*0.1*error
    
    if i%100 == 99:
        print(i, "error :", error, "output: ", output)
# error와 output이 모두 0에 가까워진다.
```
```
99 error : -0.009265281673978427 output:  0.009265281673978427
199 error : -0.008485680531408461 output:  0.008485680531408461
299 error : -0.007826671484259915 output:  0.007826671484259915
399 error : -0.007262348043585083 output:  0.007262348043585083
499 error : -0.006773695732615721 output:  0.006773695732615721
599 error : -0.006346476938848572 output:  0.006346476938848572
699 error : -0.005969825656183642 output:  0.005969825656183642
799 error : -0.005635270129538658 output:  0.005635270129538658
899 error : -0.005336134372264803 output:  0.005336134372264803
999 error : -0.005067082598395972 output:  0.005067082598395972
```
```python
# 입력이 0일 때 기대출력이 1인 뉴런은 같은 방식으로 계산하면 될까?
x = 0
y = 1
w = tf.random.normal([1],0,1)

for i in range(1000):
    output = sigmoid(x*w)
    error = y-output
    w = w + x*0.1*error
    
    if i%100 == 99:
        print(i, "error :", error, "output: ", output)
```
```
99 error : 0.5 output:  0.5
199 error : 0.5 output:  0.5
299 error : 0.5 output:  0.5
399 error : 0.5 output:  0.5
499 error : 0.5 output:  0.5
599 error : 0.5 output:  0.5
699 error : 0.5 output:  0.5
799 error : 0.5 output:  0.5
899 error : 0.5 output:  0.5
999 error : 0.5 output:  0.5
```
### 편향(bias)
- 입력이 0인 경우 경사하강법에 대입시 가중치가 업데이트 되지 않음
- bias : 입력으로 늘 한쪽으로 치우친 고정된 값을 받음. 뉴런이 아무것도 배우지 못하는 상황 방지

```python
# 편향을 사용해 다시 학습시켜보자
x = 0
y = 1
w = tf.random.normal([1],0,1)
b = tf.random.normal([1],0,1)

for i in range(1000):
    output = sigmoid(x*w + 1*b)
    error = y - output
    w = w + x*0.1*error
    # 편향의 입력으로 1을 보편적으로 사용한다
    b = b + 1*0.1*error
    
    if i%100 == 99:
        print(i, "error :", error, "output: ", output)
```
```
99 error : 0.16919710165579738 output:  0.8308028983442026
199 error : 0.06704638377521965 output:  0.9329536162247803
299 error : 0.04095145161030822 output:  0.9590485483896918
399 error : 0.029333960060246467 output:  0.9706660399397535
499 error : 0.022807911979712103 output:  0.9771920880202879
599 error : 0.018639861743502628 output:  0.9813601382564974
699 error : 0.015751615158965948 output:  0.984248384841034
799 error : 0.013633974988784048 output:  0.986366025011216
899 error : 0.01201568587394497 output:  0.987984314126055
999 error : 0.010739214542849651 output:  0.9892607854571503
```

### 첫 번째 신경망 네트워크 : AND

  
- 2개의 입력을 받을 때 AND 연산의 진리표  

|입력1|입력2|AND 연산|
|---|---|---|
|T|T|T|
|T|F|F|
|F|T|F|
|F|F|F|

- 2개의 정수입력을 받을 때 AND 연산의 진리표  

|입력1|입력2|AND 연산|
|---|---|---|
|1|1|1|
|1|0|0|
|0|1|0|
|0|0|0|


#### *2개의 입력을 받아 1개의 AND연산 출력을 계산하는 뉴런을 만들어보자*

```python
import numpy as np

x = np.array([[1,1]
             ,[1,0]
             ,[0,1]
             ,[0,0]])
y = np.array([[1],[0],[0],[0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(2000):
    error_sum = 0
    for j in range(4):
        # np.sum(x[j]*w) = x1*w1 + x2*w2
        output = sigmoid(np.sum(x[j]*w)+b_x*b)
        error = y[j][0] - output
        w = w + x[j] * 0.1 * error
        b = b + b_x * 0.1 * error
        error_sum += error
        
    if i%200 == 199:
        print(i, "error_sum :", error_sum)
```

```
199 error_sum : -0.11072222906610298
399 error_sum : -0.06585210064727257
599 error_sum : -0.04675104860677661
799 error_sum : -0.03613486697169575
999 error_sum : -0.029393160028244113
1199 error_sum : -0.024743290826010882
1399 error_sum : -0.02134701385459728
1599 error_sum : -0.018761460000190494
1799 error_sum : -0.01672726801895861
1999 error_sum : -0.01508666952750725
```
```python
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
```
```
X: [1 1] Y: [1] Output: 0.9648247424593037
X: [1 0] Y: [0] Output: 0.024926076154134537
X: [0 1] Y: [0] Output: 0.02500227392820842
X: [0 0] Y: [0] Output: 2.38985176048057e-05
```

### 두 번째 신경망 네트워크 : OR

  
- 2개의 입력을 받을 때 OR 연산의 진리표  

|입력1|입력2|OR 연산|
|---|---|---|
|T|T|T|
|T|F|T|
|F|T|T|
|F|F|F|

- 2개의 정수입력을 받을 때 OR 연산의 진리표  

|입력1|입력2|OR 연산|
|---|---|---|
|1|1|1|
|1|0|1|
|0|1|1|
|0|0|0|


#### *2개의 입력을 받아 1개의 OR연산 출력을 계산하는 뉴런을 만들어보자*

```python
x = np.array([[1,1]
             ,[1,0]
             ,[0,1]
             ,[0,0]])
y = np.array([[1],[1],[1],[0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(2000):
    error_sum=0
    for j in range(4):
        output = sigmoid(np.sum(x[j]*w)+b_x*b)
        error = y[j][0]-output
        w = w + x[j]*0.1*error
        b = b + b_x*0.1*error
        error_sum += error
    if i%200 == 199:
        print(i, "error_sum :", error_sum)
```
```
199 error_sum : -0.0447569987408202
399 error_sum : -0.024441915205087802
599 error_sum : -0.016721697312243197
799 error_sum : -0.01266855999521875
999 error_sum : -0.010179941137192658
1199 error_sum : -0.008500499420626348
1399 error_sum : -0.007292041529568451
1599 error_sum : -0.006381602222994796
1799 error_sum : -0.005672345063367033
1999 error_sum : -0.005102407413786016
```
```python
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
```
```
X: [1 1] Y: [1] Output: 0.9999972820863937
X: [1 0] Y: [1] Output: 0.9898900714045418
X: [0 1] Y: [1] Output: 0.9898518427194327
X: [0 0] Y: [0] Output: 0.025300520097864255
```
### 세 번째 신경망 네트워크 : XOR
###### 홀수 개의 입력이 참일 때만 결괏값이 참이 된다.  

   
- 2개의 입력을 받을 때 XOR 연산의 진리표  

|입력1|입력2|XOR 연산|
|---|---|---|
|T|T|F|
|T|F|T|
|F|T|T|
|F|F|F|

- 2개의 정수입력을 받을 때 XOR 연산의 진리표  

|입력1|입력2|XOR 연산|
|---|---|---|
|1|1|0|
|1|0|1|
|0|1|1|
|0|0|0|


#### *2개의 입력을 받아 1개의 XOR연산 출력을 계산하는 뉴런을 만들어보자*

```python
x = np.array([[1,1],
              [1,0],
              [0,1],
              [0,0]])
y = np.array([[0],[1],[1],[0]])
w = tf.random.normal([2],0,1)
b = tf.random.normal([1],0,1)
b_x = 1

for i in range(2000):
    error_sum=0
    for j in range(4):
        output = sigmoid(np.sum(x[j]*w) + b_x*b)
        error = y[j][0] - output
        w = w + x[j]*0.1*error
        b = b + b_x*0.1*error
        error_sum += error
    
    if i%200 == 199:
        print(i, "error_sum :", error_sum)
# 에러가 점점 줄어들다가 어느 순간 변하지 않는다.
```
```
199 error_sum : -0.001756214795933575
399 error_sum : -7.139478484707862e-05
599 error_sum : -2.9056767292257035e-06
799 error_sum : -1.3309160473706783e-07
999 error_sum : 3.7228422566926156e-09
1199 error_sum : 3.722842145670313e-09
1399 error_sum : 3.722842145670313e-09
1599 error_sum : 3.722842145670313e-09
1799 error_sum : 3.722842145670313e-09
1999 error_sum : 3.722842145670313e-09
```
```python
# 기대출력과 차이가 너무 나는데?
for i in range(4):
    print('X:', x[i], 'Y:', y[i], 'Output:', sigmoid(np.sum(x[i]*w)+b))
```
```
X: [1 1] Y: [0] Output: 0.5128176286712095
X: [1 0] Y: [1] Output: 0.5128176305326305
X: [0 1] Y: [1] Output: 0.4999999990686774
X: [0 0] Y: [0] Output: 0.5000000009313226
```
```python
# 어째서 이런 결과가 나오는지 알아보자~!
print('w:', w)
print('b:', b)
# w 값을 봤을 때 첫번째 입력이 큰 영향을 끼치는 것을 알 수 있다.
# bias는 두번째 입력과 비슷하게 거의 영향이 없다.
```
### XOR Problem
- XOR 네트워크의 가중치가 하려는 작업이 분명하지 않다.
    - 중간계산값 `np.sum(x[i]*w)+b`이 0에 가까워질 뿐임.
- 해결책은 여러개의 뉴런을 사용하는 것

### model
- `tf.keras`의 추상클래스 
- 딥러닝 계산을 위한 여러 함수와 변수의 묶음
```python
x = np.array([[1,1],
              [1,0],
              [0,1],
              [0,0]])
y = np.array([[0],[1],[1],[0]])

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=2, activation='sigmoid', input_shape=(2,)),
    tf.keras.layers.Dense(units=1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss = 'mse')

model.summary()
```
`tf.keras.Sequential`
- 뉴런과 뉴런이 합쳐진 단위인 레이어를 순차적으로 일직선으로 배치한 것
- `units=2` : 레이어를 구성하는 뉴런의 수를 정의
    - 뉴런이 많을수록 레이어의 성능은 좋아지지만 계산량이 증가하고 메모리를 많이 소모한다.
- `input_shape=(2,)` : 시퀀셜 모델의 첫 번째 레이어에서만 정의
    - 입력의 차원 수가 어떻게 되는지를 정의
    - [1,1],[1,0]처럼 2개의 입력을 받는 1차원 array이기 때문에 1차원의 원소의 개수인 2를 명시한 것임
  
`model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss = 'mse')`
- model이 실제로 동작할 수 있도록 준비하는 명령
- `optimizer=tf.keras.optimizers.SGD(lr=0.1)` : 딥러닝의 학습식을 정의하는 부분
    - `SGD` 경사하강법(Stochastic Gradient Descent)의 약자
    - `loss='mse'` : error = y - output 과 비슷

```python
# 네트워크를 실제로 학습시켜보자
history = model.fit(x, y, epochs=2000, batch_size=1)
```
```
Train on 4 samples
Epoch 1/2000
4/4 [==============================] - 0s 5ms/sample - loss: 0.2182
Epoch 2/2000
4/4 [==============================] - 0s 2ms/sample - loss: 0.2181
Epoch 3/2000
4/4 [==============================] - 0s 1ms/sample - loss: 0.2180
~~~
Epoch 1999/2000
4/4 [==============================] - 0s 2ms/sample - loss: 0.0095
Epoch 2000/2000
4/4 [==============================] - 0s 1ms/sample - loss: 0.0095
```

```python
# predict() 함수에 입력을 넣어 네트워크의 출력결과를 확인해보자
model.predict(x)
```
```
array([[0.10594922],
       [0.8989589 ],
       [0.8989161 ],
       [0.07937732]], dtype=float32)
```

```python
# 가중치와 편향을 살펴보자
for i, weight in enumerate(model.weights):
    print(i, '\n', weight)
```
```
0 
 <tf.Variable 'dense/kernel:0' shape=(2, 2) dtype=float32, numpy=
array([[-5.3315053,  3.365693 ],
       [-5.3202934,  3.3637083]], dtype=float32)>
1 
 <tf.Variable 'dense/bias:0' shape=(2,) dtype=float32, numpy=array([ 1.7690036, -5.2097464], dtype=float32)>
2 
 <tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=
array([[-6.6521096],
       [-6.581358 ]], dtype=float32)>
3 
 <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([3.2680433], dtype=float32)>
```
`model.weights`:가중치 정보 저장
- kernel : 레이어 사이의 뉴런을 연결할 때 사용되는 가중치
- bias : 편향과 연결된 가중치

### 2-레이어 XOR 네트워크의 정보 시각화
딥러닝 네트워크의 학습이 잘 되고있는지 결과가 잘 출력되는지 확인하기 위해 시각화의 필요성이 있다.

```python
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
```
