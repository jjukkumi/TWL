### 선형회귀
```python
import matplotlib.pyplot as plt

# 인구증가율과 고령인구비율 사이의 경향성을 선형회귀로 예측해보자
population_inc = [0.3, -0.78, 1.26, 0.03, 1.11, 15.17, 0.24, -0.24, -0.47, 
                  -0.77, -0.37, -0.85, -0.41, -0.27, 0.02, -0.76, 2.66]
population_old = [12.27, 14.44, 11.87, 18.75, 17.52, 9.29, 16.37, 19.78, 19.51, 
                  12.65, 14.74, 10.72, 21.94, 12.83, 15.51, 17.14, 14.42]
plt.plot(population_inc, population_old, 'bo')
plt.xlabel('Population Growth Rate')
plt.ylabel('Elderly Population Rate')
plt.show()
```

```python
# 극단치(outlier)제거
population_inc.remove(15.17)
population_old.remove(9.29)
```

```python
plt.plot(population_inc, population_old, 'bo')
plt.xlabel('Population Growth Rate')
plt.ylabel('Elderly Population Rate')
plt.show()
```

```python
# 회귀선을 구해보자
import numpy as np

X = population_inc
Y = population_old

# X와 Y의 평균을 구한다
x_bar = sum(X) / len(X)
y_bar = sum(Y) / len(Y)

# 최소제곱법으로 a와 b를 구한다
a = sum([(y-y_bar)*(x-x_bar) for y,x in list(zip(Y,X))])
a /= sum([(x-x_bar)**2 for x in X])
b = y_bar - a*x_bar
print('a:', a, 'b:', b)

# 그래프를 그리기 위해 회귀선의 x, y 데이터를 구한다
line_x = np.arange(min(X), max(X), 0.01)
line_y = a * line_x + b

# 붉은색 실선으로 회귀선을 그린다
plt.plot(line_x, line_y, 'r-')

plt.plot(X,Y,'bo')
plt.xlabel('Population Growth Rate (%)')
plt.ylabel('Elderly Population Rate (%)')
plt.show()
```

```python
# 위와같은 복잡한 수식과 최소제곱법을 쓰지 않고도 텐서플로를 이용해 회귀선을 구할 수 있다.
import tensorflow as tf
import random

# a와 b를 랜덤한 값으로 초기화한다.
a = tf.Variable(random.random())
b = tf.Variable(random.random())

# 잔차의 제곱의 평균을 반환하는 함수를 만든다.
def compute_loss():
    y_pred = a*X + b
    loss = tf.reduce_mean((Y-y_pred)**2)
    return loss

optimizer = tf.optimizers.Adam(lr=0.07)
for i in range(1000):
    # 잔차의 제곱의 평균을 최소화(minimize)한다.
    optimizer.minimize(compute_loss, var_list=[a,b])
    
    if i%100 == 99:
        print(i, 'a:', a.numpy(), 'b:', b.numpy(), 'loss:', compute_loss().numpy)

line_x = np.arange(min(X), max(X), 0.01)
line_y = a * line_x + b

# 그래프를 그린다.
plt.plot(line_x, line_y, 'r-')
plt.plot(X,Y,'bo')
plt.xlabel('Population Growth Rate (%)')
plt.ylabel('Elderly Population Rate (%)')
plt.show()
```
`optimizer.minimize(compute_loss, var_list=[a,b])`
- 1st param : 최소화할 loss
- 2nd param : 학습시킬 변수 리스트

### 다항 회귀
- 비선형회귀 : 선형 회귀로는 표현할 수 없는 데이터의 경향성을 설명하기 위한 회귀
- 비선형회귀 가운데 x^2, x^3등 다항식을 이용한 회귀를 다항회귀라고 함
- 즉 회귀선이 직선이 아닌 2차함수나 3차함수 등의 곡선이 됨  
`코딩스킵`

### 딥러닝 네트워크를 이용한 회귀
```python
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=6, activation='tanh', input_shape=(1,)),
    tf.keras.layers.Dense(units=1)
])

model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss='mse')

model.summary()
```

```python
model.fit(X,Y,epochs=10)
```

```python
line_x = np.arange(min(X),max(X),0.01)
line_y = model.predict(line_x)

plt.plot(line_x,line_y,'r-')
plt.plot(X,Y,'bo')
plt.xlabel('Population Growth Rate (%)')
plt.ylabel('Elderly Population Rate (%)')
plt.show()
```
### 보스턴 주택 가격 데이터세트
```python
from tensorflow.keras.datasets import boston_housing
(train_X, train_Y), (test_X, test_Y) = boston_housing.load_data()
print(len(train_X),len(test_X))
print(train_X[0])
print(train_Y[0])
```
#### 데이터 전처리
- 각 데이터의 단위가 다르기 때문에 정규화를 통해 학습 효율을 높인다.
- 정규화: (데이터-평균)/표준편차 -> 데이터를 정규분포로 옮겨줌
- 테스트 데이터도 **훈련 데이터의 평균과 표준편차로 정규화**한다.
```python
x_mean = train_X.mean()
x_std = train_X.std()

train_X = (train_X - x_mean)/x_std
test_X = (test_X - x_mean)/x_std

y_mean = train_Y.mean()
y_std = train_Y.std()

train_Y = (train_Y - y_mean)/y_std
test_Y = (test_Y - y_mean)/y_std

print(train_X[0])
print(train_Y[0])
```

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=52, activation='relu', input_shape=(13,)),
    tf.keras.layers.Dense(units=39, activation='relu'),
    tf.keras.layers.Dense(units=26, activation='relu'),
    tf.keras.layers.Dense(units=1)
])

model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.07),loss='mse')
model.summary()
```
`input_shape=(13,)` : x데이터의 속성을 모두 불러오기 위해 첫번째 차원을 13으로 지정  
- 마지막 레이어는 주택가격인 Y값 하나만 예측하면 되기 때문에 뉴런의 수가 1개임
```python
history = model.fit(train_X, train_Y, 
                    epochs=25, batch_size=32, validation_split=0.25)
```
`validation_split=0.25` : 훈련 데이터의 25% 정도를 검증 데이터로 분리해 학습 결과 검증
```python
plt.plot(history.history['loss'], 'b-', label='loss')
plt.plot(history.history['val_loss'], 'r--', label='val_loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()
```
- 훈련 데이터의 손실은 꾸준히 감소하지만 검증데이터의 손실은 감소하지만은 않는다.
```python
model.evaluate(test_X, test_Y)
```

```python
# 네트워크가 Y값을 얼마나 잘 예측하는지 확인하기 위해 실제 주택 가격과 예측 주택가격을 1:1로 비교해보자
pred_Y = model.predict(test_X)

plt.figure(figsize=(5,5))
plt.plot(test_Y, pred_Y, 'b.')
plt.axis([min(test_Y), max(test_Y), min(test_Y), max(test_Y)])

# y=x에 해당하는 대각선
plt.plot([min(test_Y), max(test_Y)], [min(test_Y), max(test_Y)], ls='--', c=".3")
plt.xlabel('test_Y')
plt.ylabel('pred_Y')

plt.show()
```
검증데이터와 테스트 데이터에 대해 모두 좋은 예측 성적을 내려면?
- 검증데이터와 테스트 데이터는 네트워크의 가중치에 영향을 미치지 않는다.
- 따라서 검증데이터에 대한 성적이 좋아지면 테스트 데이터에 대한 성적도 좋아질 것
- 따라서 val_loss가 높아지지 않도록 , 즉 네트워크가 훈련데이터에 과적합되지 않도록
- 학습 도중에 끼어들어 학습을 멈춰야한다.

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=52, activation='relu', input_shape=(13,)),
    tf.keras.layers.Dense(units=39, activation='relu'),
    tf.keras.layers.Dense(units=26, activation='relu'),
    tf.keras.layers.Dense(units=1)
])

model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.07), loss='mse')

history = model.fit(train_X, train_Y, 
                    epochs=25, batch_size=32, validation_split=0.25, 
                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')])
```

```python
plt.plot(history.history['loss'], 'b-', label='loss')
plt.plot(history.history['val_loss'], 'r--', label='val_loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()
```

```python
model.evaluate(test_X, test_Y)
```

```python
pred_Y = model.predict(test_X)

plt.figure(figsize=(5,5))
plt.plot(test_Y, pred_Y, 'b.')
plt.axis([min(test_Y), max(test_Y), min(test_Y), max(test_Y)])

plt.plot([min(test_Y), max(test_Y)],[min(test_Y), max(test_Y)],ls='--', c='.3')
plt.xlabel('test_Y')
plt.ylabel('pred_Y')

plt.show()
```
#### 콜백 함수로 EarlyStopping을 이용해 네트워크가 과적합되지 않도록 도중에 학습 멈춤
#### EarlyStopping을 적용하기 전 모델보다 좀 더 다양한 예측을 함